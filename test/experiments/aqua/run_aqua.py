import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import time
import traceback
import json
import re
from typing import List, Tuple, Dict
import numpy as np
from tqdm import tqdm
import asyncio


from AgentDropout.agents import AgentRegistry
from AgentDropout.agents.supervisor_reasoning_pick_metric import Supervisor
from AgentDropout.agents.final_decision import FinalRefer
from autogen_agentchat.teams import SelectorGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.ui import Console


from project_datasets.aqua_dataset import aqua_get_predict, aqua_data_process


# ==============================================================================
def log_message(msg: str, log_file: str = None):
    print(msg)
    if log_file:
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(msg + "\n")


# ==============================================================================
def load_global_resources(metric_file, cache_file):
    print(f"Loading Global Resources...")
    print(f" - Metrics: {metric_file}")
    print(f" - Cache:   {cache_file}")
    
    with open(metric_file, "r", encoding='utf-8') as f:
        metrics = json.load(f)
        
    emb_map = {}
    if os.path.exists(cache_file):
        with open(cache_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        record = json.loads(line)
                        emb_map[record["name"]] = record["vector"]
                    except: pass
    else:
        print(f"[Warning] Cache file not found: {cache_file}")
                
    vectors_list = []
    for m in metrics:
        name = m['name']
        if name in emb_map:
            vectors_list.append(np.array(emb_map[name], dtype=np.float32))
        else:
            if len(emb_map) > 0:
                vectors_list.append(np.zeros(len(next(iter(emb_map.values()))), dtype=np.float32))
            else:
                vectors_list.append(np.array([]))
            
    embeddings = np.stack(vectors_list) if vectors_list else np.array([])
    print(f"Global Resources Loaded. Embedding Shape: {embeddings.shape}")
    
    return metrics, embeddings


# ==============================================================================
def init_team(preloaded_metrics, preloaded_embeddings) -> Tuple[SelectorGroupChat, FinalRefer, Dict[str, str], Supervisor]:
    
    supervisor = Supervisor(
        model=args.supervisor_model,
        api_key="EMPTY",
        base_url=args.supervisor_url,
        domain="aqua", # AQuA Domain
        pass_rate=args.pass_rate,
        prune_flag=True,
        
        embedding_api_key="EMPTY",
        embedding_model=args.embedding_model,
        embedding_api_base=args.embedding_url,
        
        preloaded_metrics=preloaded_metrics,
        preloaded_embeddings=preloaded_embeddings,
        
        direct_k=args.direct_k,
        use_simple_audit=int(args.use_simple_audit),
        random_k=args.random_k,
    )

    agent_resgistry = AgentRegistry()
    participants = [
        agent_resgistry.get(
            agent_name="MathSolver_aqua",
            name=f"Participant_{i + 1}",
            domain="aqua",
            model=args.reasoning_model,
            api_key="EMPTY",
            base_url=args.reasoning_url,
            supervisor=supervisor,
            reflection_time=args.retries_times,
        ) for i in range(5)
    ]

    role_map = {agent.name: agent.role for agent in participants}
    for agent in participants:
        agent.role_map = role_map
        
    selector_prompt = """Select an agent to perform task.
{roles}
Current conversation context:
{history}
Read the above conversation, then select an agent from {participants} to perform the next task.
Make sure the planner agent has assigned tasks before other agents start working.
Only select one agent.
"""

    model_client = OpenAIChatCompletionClient(
        model="####", 
        api_key="####", 
        base_url="####"
    )
    
    text_mention_termination = TextMentionTermination("TERMINATE")
    max_messages_termination = MaxMessageTermination(max_messages=args.max_turns)
    termination = text_mention_termination | max_messages_termination

    team = SelectorGroupChat(
        participants=participants,
        model_client=model_client,
        termination_condition=termination,
        selector_prompt=selector_prompt,
        allow_repeated_speaker=True, 
    )

    decision_maker = AgentRegistry.get(
        agent_name="FinalRefer",
        name="DecisionMaker",
        domain="aqua",
        model=args.reasoning_model,
        api_key="EMPTY",
        base_url=args.reasoning_url
    )
    
    return team, decision_maker, role_map, supervisor


# ==============================================================================
async def reasoning(question, team: SelectorGroupChat, decision_maker: FinalRefer, role_map: Dict[str, str], supervisor: Supervisor):
    
    is_baseline_mode = getattr(args, 'baseline_only', False)
    
    if is_baseline_mode:
        print(f"\n>>> [Mode] Baseline Only (No Audit / No Pruning)")
        await team.reset()
        supervisor.reset()
        supervisor.prune_flag = False 
        await Console(team.run_stream(task=question))
        history_messages = supervisor.get_messages_above_threshold()

    else:
        print("\n>>> [Phase 1] ...")
        await team.reset()
        supervisor.reset()
        supervisor.prune_flag = True 
        await Console(team.run_stream(task=question))
        
        history_messages = supervisor.get_messages_above_threshold()
        retained_count = len(history_messages)
        print(f"\n[Check]save: {retained_count}")

        if retained_count <= 1:
            print(f"\nâš ï¸  (Fallback Triggered)ï¼")
            print(">>> [Phase 2]  Vanilla AutoGen...")
            await team.reset()
            supervisor.reset()
            original_prune_flag = supervisor.prune_flag
            supervisor.prune_flag = False 
            await Console(team.run_stream(task=question))
            history_messages = supervisor.get_messages_above_threshold()
            supervisor.prune_flag = original_prune_flag
        
    print("\n" + "="*50)
    print("--- [DEBUG] Final Decision  ---")
    
    if not history_messages:
        print("  >> !")
    else:
        for i, msg in enumerate(history_messages):
            if msg.source != 'user':
                print(f"  -  {i+1} | save: {msg.source}")

    raw_answer = await decision_maker.run_decision(history_messages=history_messages, role_map=role_map, task=question)
    raw_content = raw_answer.content.strip()
    
    print("\n[DEBUG] 2. Final Decision :")
    print(raw_content[:200] + "...")


    final_answer = aqua_get_predict(raw_content)
    print(f"  >> (hypothesis): \"{final_answer}\"")
    
    print("="*50 + "\n")
    
    ret_scores = supervisor.get_scores(role_map)
    reflection_records = getattr(supervisor, 'reflection_records', [])

    return final_answer, raw_content, ret_scores, reflection_records


def write_to_file(out_file, data_id, data):
    exist_data = {}
    if os.path.exists(out_file):
        try:
            with open(out_file, 'r') as f:
                exist_data = json.load(f)
        except json.JSONDecodeError:
            pass
            
    exist_data[str(data_id)] = data

    try:
        sorted_keys = sorted(exist_data.keys(), key=lambda x: int(x))
    except:
        sorted_keys = sorted(exist_data.keys())
        
    sorted_data = {key: exist_data[key] for key in sorted_keys}
    
    with open(out_file, 'w') as f:
        json.dump(sorted_data, f, indent=4, ensure_ascii=False)
    

async def run_sample(data, out_file, team, decision_maker, role_map, supervisor, log_file_path):
    instance_id = data.get('id', 'unknown')
    
    try:
        current_time = time.strftime('%Y-%m-%d %H:%M:%S')
        log_message(f"\n{'='*40}", log_file_path)
        log_message(f"=== TASK ID: {instance_id} ===", log_file_path)
        log_message(f"{'='*40}", log_file_path)
        log_message(f"--- [ {current_time} ] Start Processing ---", log_file_path)
        
        answer, raw_content, scores, reflection_records = await reasoning(data['task'], team, decision_maker, role_map, supervisor)
        

        is_correct = (answer == data['answer'])
        
        log_message(f"over: {instance_id} | Correct: {is_correct} (GT: {data['answer']} vs Pred: {answer})", log_file_path)
        
        write_to_file(
            out_file, 
            instance_id, 
            {
                'id': instance_id,
                'answer': data['answer'], 
                'hypothesis': answer, 
                'question': data['task'], 
                'raw_response': raw_content,
                'is_correct': is_correct,
                'scores': scores,
                'reflection_records': reflection_records
            }
        )
        
    except Exception as e:
        log_message(f"!!!!!! [CRITICAL ERROR] Task {instance_id}: {e} !!!!!!", log_file_path)
        traceback.print_exc() 


async def main():
    if not os.path.exists(args.in_file):
        print(f"[ERROR] Input file not found: {args.in_file}")
        return

    print(f"[INFO] Loading data from {args.in_file}...")
    with open(args.in_file, 'r') as f:
        input_lines = f.readlines()
        input_data = [json.loads(line) for line in input_lines]
    

    input_data = aqua_data_process(input_data)
    
    if args.limit is not None and args.limit > 0:
        print(f"[INFO] ðŸ”§ Limit applied: {args.limit} tasks.")
        input_data = input_data[:args.limit]
    
    global_metrics, global_embeddings = load_global_resources(
        args.metric_pool_file, 
        args.embedding_cache_file
    )
    
    if args.log_file:
        FINAL_LOG_FILE = args.log_file
    else:
        base_dir = os.path.dirname(args.out_file)
        base_name = os.path.basename(args.out_file).replace(".json", "_full.log")
        FINAL_LOG_FILE = os.path.join(base_dir, base_name)
        
    os.makedirs(os.path.dirname(FINAL_LOG_FILE), exist_ok=True)
    
    with open(FINAL_LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"=== AQuA Run Logs ===\n")
        f.write(f"Total: {len(input_data)}\n\n")

    
    start_time = time.time()
    

    for instance in tqdm(input_data, desc="Processing"):
        team, decision_maker, role_map, supervisor = init_team(global_metrics, global_embeddings)
        await run_sample(instance, args.out_file, team, decision_maker, role_map, supervisor, FINAL_LOG_FILE)
    
    total_time = time.time() - start_time
    print(f"\nðŸŽ‰ : {total_time:.2f}s")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--in_file', type=str)
    parser.add_argument('--out_file', type=str)
    parser.add_argument('--reasoning_url', type=str)
    parser.add_argument('--reasoning_model', type=str)
    parser.add_argument('--supervisor_url', type=str)
    parser.add_argument('--supervisor_model', type=str)
    
    parser.add_argument('--embedding_url', type=str, required=True)
    parser.add_argument('--embedding_model', type=str, required=True)
    parser.add_argument("--metric_pool_file", type=str, required=True)
    parser.add_argument("--embedding_cache_file", type=str, required=True)
    
    parser.add_argument('--max_turns', type=int)
    parser.add_argument('--metrics_retrieve_k', type=int)
    parser.add_argument('--pass_rate', type=float, default=0.8)
    parser.add_argument('--retries_times', type=int, default=3)
    parser.add_argument('--log_file', type=str)
    
    parser.add_argument("--limit", type=int, default=None)
    parser.add_argument("--baseline_only", action="store_true")
    
    parser.add_argument("--use_simple_audit", action="store_true", help="Enable Simple Audit")
    
    parser.add_argument("--direct_k", type=int, default=5)
    parser.add_argument("--random_k", type=int, default=0)
    
    args = parser.parse_args()
    
    os.makedirs(os.path.dirname(args.out_file), exist_ok=True)
    
    asyncio.run(main())